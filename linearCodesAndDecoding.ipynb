{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d87b773",
   "metadata": {},
   "source": [
    "# OSD decoding\n",
    "Fix $n$ and a linear binary code, $\\mathcal{C}$ of dimension $k$. So $\\mathcal{C} \\subset F^n_2$ such that $\\mathcal{C} \\cong F_2^k$ and in particular has $2^k$ elements. \n",
    "Choose a generator matrix $G$, meaning a matrix which row space spans the code, and a parity matrix $H$, namely a matrix which kernel is the code.\n",
    "By definition of the parity matrix, if $c\\in \\mathcal{C}$ then $H \\cdot c = \\vec{0}$. Generally we call the resulting vector $H \\cdot w = s$ the syndrome. \n",
    "\n",
    "Excersize:\n",
    "Show that if $v,w$ have the same syndrome, then they differ by a codeword.\n",
    "\n",
    "For a set of indices, $I \\subset {1,..n} = [n]$, we say that $I$ is an information set for the code $\\mathcal{C}$, if:\n",
    "\n",
    "$$C_I:=\\{c_{i\\in I} : c\\in \\mathcal{C}\\} = F_2^k$$\n",
    "\n",
    "Meaning, that when we restrict each $n$ dimensional codeword $c$ to the set of indices in $I$, and go over all codewords in $\\mathcal{C}$, we get the set $F_2^k$, and in fact we get an isomorphism of vector spaces between $\\mathcal{C}$ and $F_2^k$ which is a projection.\n",
    "\n",
    "Excersize:\n",
    "1.  Show that for an information set $I$, the map: $P_I: \\mathcal{C} \\rightarrow F_2^k$ defined by: $P_I(c) = P_I(c_{i\\in [n]}) = c_{i\\in I}$ is linear, one to one and onto, i.e., it is an isomorphism of vector spaces.\n",
    "2. Conclude that if $P_I(c) = P_I(c')$ then $c=c'$\n",
    "\n",
    "Every pair of information set and syndrome for the code $\\mathcal{C}$ and $H$ gives us an encoding $E(x) = E(s,I,x):F_2^k \\rightarrow F^n_2$: first, the bijection $P_I$ picks one representative of $\\mathcal{C}$ - the one that agrees with x over all indices in $I$. Then the rest of the values for indices outside $I$ are uniqely determined by the syndrome.\n",
    "\n",
    "\n",
    "Now let's assume we have a word $c' = c + e$, and we want to recover $c$. We also have an estimate $\\hat{e}$ of $e$, but we find that $H \\cdot \\hat{e} \\neq H \\cdot e$, so we have evidence that $\\hat{e} \\neq e$. \n",
    "If we knoew of an information set $I$ such that $e_I = \\hat{e}_I$, i.e., they agree on the set $I$, then the encoding $E(s = H\\cdot e,I,\\hat{e})$ would find us the right $e$.\n",
    "\n",
    "Suppose further that our estimate of $\\hat{e}$ was cooked from probabilities $p_i$, where $p_i$ is the probability of the error $e$ being $1$ at index $i$, i.e.:\n",
    "$$p_i = P(e_i = 1)$$\n",
    "And we decide that we estimate $\\hat{e}$ using: $e_i = 1$ if $p_i >= 1/2$ and 0 otherwise.\n",
    "\n",
    "This is why we introduce the notion of a reliability $\\rho$ of an index $i$:\n",
    "$$\\rho_i = P(\\hat{e}_i = e_i)$$\n",
    "Which using our estimation rule means \n",
    "$$\\rho_i = max(p_i, 1-p_i)$$\n",
    "(since if $p_i$ is the bigger one, we decided $\\hat{e_i}$ = 1, and otherwise, i.e. $1-p_i$  is bigger, and we decided $\\hat{e_i}=0$ but either way we are as certain as $max(p_i, 1-p_i)$). \n",
    "\n",
    "\n",
    "\n",
    "BIN:\n",
    "\n",
    "\n",
    "\n",
    "So suppose we knew, that the error part of $x = c + e$ occures outside some information set $J$, then we could find out the error precisely usining $E(s,J,\\vec{0})$ for $s=H \\cdot (c+e) = H \\cdot e$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f91fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
